from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize
import spacy
from keras.preprocessing.text import text_to_word_sequence
from gensim.utils import tokenize
text = """Founded in 2002, SpaceX’s mission is to enable humans to become 
a spacefaring civilization
and a multi-planet species by building a self-sustaining city on Mars. In 
2008, SpaceX’s Falcon 1 became
the first privately developed
liquid-fuel launch vehicle to orbit the Earth."""
print("\n\nGenism Tokenization:",list(tokenize(text)))

from nltk.tokenize import RegexpTokenizer  #reg(exp)
from nltk.tokenize import word_tokenize
import spacy
from keras.preprocessing.text import text_to_word_sequence
from gensim.utils import tokenize
text = """Founded in 2002, SpaceX’s mission is to enable humans to become
a spacefaring civilization
and a multi-planet species by building a selfsustaining city on Mars. In 2008, SpaceX’s Falcon 1 became
the
first privately developed liquid-fuel launch vehicle to orbit the Earth."""
tk = RegexpTokenizer('\s+', gaps = True)
regex_token = tk.tokenize(text)
print("\n\nRegular expression tokenizer: ",regex_token)

!pip install genism  #split
!pip install keras
!pip install spacy
!pip install tensorflow
#python -m pip install --upgrade pip # if tensorflow package error come
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize
import spacy
from keras.preprocessing.text import text_to_word_sequence
from gensim.utils import tokenize
text = """Founded in 2002, SpaceX’s mission is to enable humans to become 
a spacefaring civilization and a multi-planet species by building a selfsustaining city on Mars. In 2008, SpaceX’s Falcon 1 becamethefirst privately 
developed liquid-fuel launch vehicle to orbit the Earth."""
split_token=text.split()
print("Tokenization using split function: ",split_token)

import nltk
from nltk.tokenize import RegexpTokenizer
# Sentence in Hindi
sentence = "मेरा नाम जॉन है। मैंभारत मेंरहता हूँ।"
# Regular expression pattern for Hindi words
pattern = r'\w+'
# Initialize the tokenizer with the pattern
tokenizer = RegexpTokenizer(pattern)
# Tokenize the sentence into words
tokens = tokenizer.tokenize(sentence)
# Print the tokens
for token in tokens:
 print(token)
